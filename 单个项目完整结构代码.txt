é¡¹ç›® 'deno-aianswergenerator-2api' çš„ç»“æ„æ ‘:
ğŸ“‚ deno-aianswergenerator-2api/
    ğŸ“„ .env
    ğŸ“„ main.ts
================================================================================

--- æ–‡ä»¶è·¯å¾„: .env ---

# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=1

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8090

--- æ–‡ä»¶è·¯å¾„: main.ts ---

// main.ts
// A Deno-powered, single-file implementation of the aianswergenerator-2api project.

// --- 1. DEPENDENCY IMPORTS ---
// Deno Standard Library for environment variable loading and colored logging
import { load } from "https://deno.land/std@0.224.0/dotenv/mod.ts";
import { green, yellow, cyan, red } from "https://deno.land/std@0.224.0/fmt/colors.ts";

// --- 2. CONFIGURATION ---
// Load environment variables from .env file
const env = await load({ export: true });

const config = {
  APP_NAME: "aianswergenerator-2api (Deno Version)",
  APP_VERSION: "1.0.0",
  DESCRIPTION: "ä¸€ä¸ªå°† aianswergenerator.pro çš„åç«¯ API (pollinations.ai) è½¬æ¢ä¸ºå…¼å®¹ OpenAI æ ¼å¼çš„ä»£ç†ã€‚",
  API_MASTER_KEY: Deno.env.get("API_MASTER_KEY") || "1",
  API_REQUEST_TIMEOUT: 120 * 1000, // in milliseconds
  NGINX_PORT: parseInt(Deno.env.get("NGINX_PORT") || "8090", 10),
  PSEUDO_STREAM_DELAY: 0.01 * 1000, // in milliseconds
  DEFAULT_MODEL: "aianswergenerator-openai",
  KNOWN_MODELS: ["aianswergenerator-openai"],
  UPSTREAM_MODEL_PARAM: "openai",
};

// --- 3. LOGGER UTILITY ---
const logger = {
  _format(level: string, color: (str: string) => string, msg: string) {
    const time = new Date().toISOString().replace("T", " ").replace("Z", "");
    console.log(
      `${green(time)} | ${color(level.padEnd(8))} | ${cyan("main.ts")} - ${msg}`,
    );
  },
  info(msg: string) {
    this._format("INFO", green, msg);
  },
  error(msg: string) {
    this._format("ERROR", red, msg);
  },
  success(msg: string) {
    this._format("SUCCESS", green, msg);
  },
};

// --- 4. SSE (SERVER-SENT EVENTS) UTILITIES ---
namespace SSE {
  const textEncoder = new TextEncoder();
  export const DONE_CHUNK = textEncoder.encode("data: [DONE]\n\n");

  export function createSseData(data: Record<string, unknown>): Uint8Array {
    const jsonString = JSON.stringify(data);
    return textEncoder.encode(`data: ${jsonString}\n\n`);
  }

  export function createChatCompletionChunk(
    requestId: string,
    model: string,
    content: string,
    finishReason: string | null = null,
  ): Record<string, unknown> {
    return {
      id: requestId,
      object: "chat.completion.chunk",
      created: Math.floor(Date.now() / 1000),
      model: model,
      choices: [
        {
          index: 0,
          delta: { content: content },
          finish_reason: finishReason,
        },
      ],
    };
  }
}

// --- 5. CORE LOGIC PROVIDER ---
class AIAnswerGeneratorProvider {
  private readonly BASE_URL = "https://text.pollinations.ai";

  private _prepareHeaders(): Headers {
    return new Headers({
      "Accept": "*/*",
      "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
      "Origin": "https://aianswergenerator.pro",
      "Referer": "https://aianswergenerator.pro/",
      "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    });
  }

  async getModels(): Promise<Response> {
    const modelData = {
      object: "list",
      data: config.KNOWN_MODELS.map((name) => ({
        id: name,
        object: "model",
        created: Math.floor(Date.now() / 1000),
        owned_by: "lzA6",
      })),
    };
    return Response.json(modelData);
  }

  async chatCompletion(requestData: any): Promise<Response> {
    const stream = new ReadableStream({
      async start(controller) {
        const requestId = `chatcmpl-${crypto.randomUUID()}`;
        const modelName = requestData.model || config.DEFAULT_MODEL;

        try {
          const messages = requestData.messages || [];
          const lastUserMessage = messages.slice().reverse().find(
            (m: any) => m.role === "user",
          )?.content;

          if (!lastUserMessage) {
            throw new Error("æœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯ã€‚");
          }

          const encodedPrompt = encodeURIComponent(lastUserMessage);
          const upstreamUrl = `${this.BASE_URL}/${encodedPrompt}?model=${config.UPSTREAM_MODEL_PARAM}`;
          const headers = this._prepareHeaders();

          logger.info(`è¯·æ±‚ä¸Šæ¸¸ URL: GET ${upstreamUrl}`);
          
          const response = await fetch(upstreamUrl, {
            method: "GET",
            headers: headers,
            signal: AbortSignal.timeout(config.API_REQUEST_TIMEOUT),
          });

          if (!response.ok) {
            const errorText = await response.text();
            throw new Error(`ä¸Šæ¸¸æœåŠ¡é”™è¯¯: ${response.status} - ${errorText}`);
          }

          const fullText = await response.text();
          logger.info(`æ”¶åˆ°ä¸Šæ¸¸å®Œæ•´å“åº”ï¼Œé•¿åº¦: ${fullText.length} characters.`);
          logger.info("å¼€å§‹æ‰§è¡Œä¼ªæµå¼ç”Ÿæˆ...");

          for (const char of fullText) {
            const chunk = SSE.createChatCompletionChunk(requestId, modelName, char);
            controller.enqueue(SSE.createSseData(chunk));
            await new Promise(resolve => setTimeout(resolve, config.PSEUDO_STREAM_DELAY));
          }

          const finalChunk = SSE.createChatCompletionChunk(requestId, modelName, "", "stop");
          controller.enqueue(SSE.createSseData(finalChunk));
          controller.enqueue(SSE.DONE_CHUNK);
          logger.success("ä¼ªæµå¼ç”Ÿæˆå®Œæˆã€‚");

        } catch (e) {
          const errorMessage = e instanceof Error ? e.message : "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯";
          logger.error(`å¤„ç†æµæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: ${errorMessage}`);
          const errorChunk = SSE.createChatCompletionChunk(requestId, modelName, errorMessage, "stop");
          controller.enqueue(SSE.createSseData(errorChunk));
          controller.enqueue(SSE.DONE_CHUNK);
        } finally {
          controller.close();
        }
      },
      _prepareHeaders: this._prepareHeaders, // Pass context to the stream
      BASE_URL: this.BASE_URL,
    });

    return new Response(stream, {
      headers: {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
      },
    });
  }
}

// --- 6. SERVER & ROUTING ---
async function mainHandler(req: Request): Promise<Response> {
  const url = new URL(req.url);
  const provider = new AIAnswerGeneratorProvider();

  // --- Security Middleware ---
  if (config.API_MASTER_KEY && config.API_MASTER_KEY !== "1") {
    const authHeader = req.headers.get("Authorization");
    if (!authHeader || !authHeader.toLowerCase().startsWith("bearer ")) {
      return Response.json({ detail: "éœ€è¦ Bearer Token è®¤è¯ã€‚" }, { status: 401 });
    }
    const token = authHeader.substring(7);
    if (token !== config.API_MASTER_KEY) {
      return Response.json({ detail: "æ— æ•ˆçš„ API Keyã€‚" }, { status: 403 });
    }
  }

  // --- Router ---
  if (req.method === "GET" && url.pathname === "/") {
    return Response.json({
      message: `æ¬¢è¿æ¥åˆ° ${config.APP_NAME} v${config.APP_VERSION}. æœåŠ¡è¿è¡Œæ­£å¸¸ã€‚`,
    });
  }

  if (req.method === "GET" && url.pathname === "/v1/models") {
    return provider.getModels();
  }

  if (req.method === "POST" && url.pathname === "/v1/chat/completions") {
    try {
      const requestData = await req.json();
      return provider.chatCompletion(requestData);
    } catch (e) {
      logger.error(`å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: ${e.message}`);
      return Response.json({ detail: `å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: ${e.message}` }, { status: 500 });
    }
  }

  return Response.json({ detail: "Not Found" }, { status: 404 });
}

// --- 7. MAIN EXECUTION ---
logger.info(`åº”ç”¨å¯åŠ¨ä¸­... ${config.APP_NAME} v${config.APP_VERSION}`);
logger.info("æœåŠ¡å·²è¿›å…¥ 'Pseudo-Stream' æ¨¡å¼ã€‚");
logger.info(`æœåŠ¡å°†åœ¨ http://localhost:${config.NGINX_PORT} ä¸Šå¯ç”¨`);

Deno.serve({ port: config.NGINX_PORT }, mainHandler);



